"""
Postprocessing commands for ensemble WRF output
"""

import re
import sys
from pathlib import Path
from typing import Optional

import click
import xarray as xr

from wrf_ensembly import experiment, processors
from wrf_ensembly.click_utils import GroupWithStartEndPrint, pass_experiment_path
from wrf_ensembly.console import logger
from wrf_ensembly.postprocess.streaming_pipeline import (
    process_cycle_single_member,
    process_cycle_streaming,
)
from wrf_ensembly.postprocess.utils import apply_compression


@click.group(name="postprocess", cls=GroupWithStartEndPrint)
def postprocess_cli():
    """Postprocessing commands for ensemble output."""
    pass


@postprocess_cli.command()
@pass_experiment_path
def print_variables_to_keep(experiment_path: Path):
    """
    Prints which variables will be kept in the wrfout files after postprocessing.

    This is useful to check if the variables you want to keep are actually kept
    or to check how the regex filters are applied.

    The variables are defined in the `PostprocessConfig:variables_to_keep` variable
    of the configuration file.

    Cycle 0 output for member 0 must exist for this command to work.
    """

    logger.setup("postprocess-print_variables_to_keep", experiment_path)
    exp = experiment.Experiment(experiment_path)

    if not exp.cfg.postprocess.variables_to_keep:
        logger.warning("`variables_to_keep` is not set in the config, exiting")
        sys.exit(0)

    # Find a sample forecast file, cycle 0, member 0
    scratch_forecast_dir = exp.paths.scratch_forecasts_path(0)
    forecast_files = list(scratch_forecast_dir.rglob("member_00/wrfout*"))
    if len(forecast_files) == 0:
        logger.error("No forecast files found, exiting")
        sys.exit(1)

    # Use the first file
    forecast_file = forecast_files[0]
    logger.info(f"Using {forecast_file} as sample file")
    ds = xr.open_dataset(forecast_file, decode_times=False)

    # Add variables that would normally be generated by xwrf_post
    for new_var in [
        "air_potential_temperature",
        "air_pressure",
        "wind_east",
        "wind_north",
        "air_density",
    ]:
        ds[new_var] = xr.zeros_like(ds["THM"])
    ds = ds.rename({"XTIME": "t"})

    filters = [re.compile(x) for x in exp.cfg.postprocess.variables_to_keep]
    logger.info("Variables in output file:")
    for var in ds.data_vars:
        for f in filters:
            if f.match(str(var)):
                logger.info(f"{f} -> {var}: ({ds[var].dims}) ({ds[var].dtype})")
                break


@postprocess_cli.command()
@click.option(
    "--cycle",
    type=int,
    help="Cycle to process. Defaults to current cycle.",
)
@click.option(
    "--only-last-timestep",
    is_flag=True,
    default=False,
    help="Only process the last timestep of the cycle.",
)
@pass_experiment_path
def run(
    experiment_path: Path,
    cycle: Optional[int],
    only_last_timestep: bool,
):
    """
    Run the complete postprocessing pipeline for a single cycle.

    Processes all ensemble members through the configured processor pipeline,
    computes ensemble statistics (mean and standard deviation), and writes
    final output files directly without intermediate files.

    This command processes ONE cycle at a time with no side effects, making it
    safe to run multiple instances in parallel for different cycles:

    \b
        # Using GNU parallel
        parallel -j 4 wrf_ensembly postprocess run --cycle {} ::: {0..23}

    \b
        # Using SLURM array jobs
        #SBATCH --array=0-23
        wrf_ensembly postprocess run --cycle $SLURM_ARRAY_TASK_ID
    """

    logger.setup("postprocess-run", experiment_path)
    exp = experiment.Experiment(experiment_path)

    if cycle is None:
        cycle = exp.current_cycle_i

    logger.info(f"Processing cycle {cycle}: {exp.cycles[cycle]}")

    # Create processor pipeline
    try:
        pipeline = processors.create_pipeline_from_config(
            exp.cfg.postprocess.processors
        )
        logger.info(
            f"Created pipeline with processors: {[p.name for p in pipeline.processors]}"
        )
    except Exception as e:
        logger.error(f"Failed to create processor pipeline: {e}")
        sys.exit(1)

    n_members = exp.cfg.assimilation.n_members
    logger.info(f"Ensemble size: {n_members} members")

    # Choose processing function based on ensemble size
    if n_members == 1:
        logger.info("Single member ensemble - skipping statistics computation")
        process_func = process_cycle_single_member
    else:
        process_func = process_cycle_streaming

    # Process forecast files
    logger.info("Processing forecast files...")
    forecast_result = process_func(
        exp, cycle, pipeline, source="forecast", only_last_timestep=only_last_timestep
    )
    if forecast_result is None:
        logger.warning("No forecast files found to process")
    else:
        if isinstance(forecast_result, tuple):
            logger.info(
                f"Forecast output: {forecast_result[0].name}, {forecast_result[1].name}"
            )
        else:
            logger.info(f"Forecast output: {forecast_result.name}")

    # Process analysis files
    logger.info("Processing analysis files...")
    analysis_result = process_func(
        exp, cycle, pipeline, source="analysis", only_last_timestep=only_last_timestep
    )
    if analysis_result is None:
        logger.warning("No analysis files found to process")
    else:
        if isinstance(analysis_result, tuple):
            logger.info(
                f"Analysis output: {analysis_result[0].name}, {analysis_result[1].name}"
            )
        else:
            logger.info(f"Analysis output: {analysis_result.name}")

    # Apply NCO compression if configured
    if exp.cfg.postprocess.compression_filters or exp.cfg.postprocess.ppc_filter:
        logger.info("Applying NCO compression to output files...")
        apply_compression(exp, cycle)

    logger.info(f"Cycle {cycle} postprocessing complete")


@postprocess_cli.command()
@click.option(
    "--cycle",
    type=int,
    help="Cycle to clean up. Defaults to current cycle.",
)
@click.option(
    "--remove-wrfout/--keep-wrfout",
    default=True,
    help="Remove the raw wrfout files (default: remove).",
)
@pass_experiment_path
def clean(experiment_path: Path, cycle: Optional[int], remove_wrfout: bool):
    """
    Clean up the scratch directory for the given cycle.

    Use after running the postprocessing pipeline to save disk space.
    By default, removes raw wrfout files. Use --keep-wrfout to preserve them.
    """

    logger.setup("postprocess-clean", experiment_path)
    exp = experiment.Experiment(experiment_path)

    if cycle is None:
        cycle = exp.current_cycle_i

    logger.info(f"Cleaning scratch for cycle {cycle}")

    scratch_dirs = [
        exp.paths.scratch_forecasts_path(cycle),
        exp.paths.scratch_analysis_path(cycle),
    ]

    for scratch_dir in scratch_dirs:
        if not scratch_dir.exists():
            continue

        if remove_wrfout:
            for f in scratch_dir.rglob("wrfout*"):
                logger.info(f"Removing {f}")
                f.unlink()
